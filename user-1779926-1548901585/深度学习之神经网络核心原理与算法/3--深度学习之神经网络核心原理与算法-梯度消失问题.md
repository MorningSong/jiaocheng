## 梯度消失问题

梯度消失问题是在早期的bp网络中比较常见的一个问题。一旦发生了梯度消失的问题，我们的训练就很难继续下去。训练不再收敛，也就是loss不再下降，准确率过早的无法提高。


![mark](http://myphoto.mtianyan.cn/blog/180331/AHbJ7c5di5.png?imageslim)

这是一个简单的神经元首尾相连组成一个神经网络。位于网络前部的w1在更新的时候，需要计算损失函数对w1的偏导。

我们根据链式法则可以得到w1的偏导的表达式

![mark](http://myphoto.mtianyan.cn/blog/180331/iDIliD5hj9.png?imageslim)

第一项。和第三项实际是在找sigmoid函数上的斜率。

![mark](http://myphoto.mtianyan.cn/blog/180331/K3b7JIBAal.png?imageslim)

![mark](http://myphoto.mtianyan.cn/blog/180331/CBahJIIGCC.png?imageslim)

sigmoid函数在大于4或者小于负4时，它的导数，导数就是函数的切线斜率接近于0.

这两项式子只要任何一个处于大于4或者小于负4就会造成导数值接近于0.曲线的斜率接近水平了。

这个接近0的值连乘时会乘出一个非常小的数。当你的网络层数很多时，越往前传情况越糟糕。w的变化会越来越慢。导致这层的w没有学到什么东西。

这就是梯度消失，或者叫梯度弥散问题。

### 如何避免梯度消失问题。

我们刚才说导数小，导致每次更新的时候的值过于的小，是不是导数大每次更新的值就会大呢，网络的学习速度就会快呢？

如果你需要导数大，最好是:

![mark](http://myphoto.mtianyan.cn/blog/180331/jgcdAK2gmB.png?imageslim)

在这个链式相乘的法则中每一项绝对值大于1.小于1，很小的数乘以很小的数会导致越乘越小。就会导致靠w的变化速度越来越慢，学习到的越来越低。

我们再来关系一下第二项和第三项。

![mark](http://myphoto.mtianyan.cn/blog/180331/Aj3dLb5KCK.png?imageslim)

第二项把z=wx+b带入。第三项其实就是对sigmoid函数求导(sigmoid函数的特性)

![mark](http://myphoto.mtianyan.cn/blog/180331/9db5I6ALik.png?imageslim)

![mark](http://myphoto.mtianyan.cn/blog/180331/GjCE00Jg0b.png?imageslim)

我们的想法是: 消除链式法则中发生连乘时每一项绝对值小于1的情况。

![mark](http://myphoto.mtianyan.cn/blog/180331/LJ6lGlg3aH.png?imageslim)

### 方法1

初始化一个合适的w，我们可以把w的值初始化的大一些。

把x=0带入方程:

![mark](http://myphoto.mtianyan.cn/blog/180331/H1iE4FIl0e.png?imageslim)

![mark](http://myphoto.mtianyan.cn/blog/180331/I8daD3d52c.png?imageslim)

![mark](http://myphoto.mtianyan.cn/blog/180331/F3h0ffd1dA.png?imageslim)

看上去满足了我们的要求，链式相乘绝对值大于1.

>梯度爆炸。原来是因为因为前面的w1变化太慢而导致梯度消失的问题发生。
现在是网络前端的这个变化率太高了。一次变化量很大，网络层数很多，有十层。十个2.5相乘就是9536

这样会造成你挪动的步子太大了，梯度爆炸。

>因此我们给w初始化一个比较大的初始值，这个是不太可行的。


## 方法2

我们要使用Relu函数来解决我们的梯度消失和爆炸的问题。

>解决这个问题的方法也就是使用导数值比较合适的激励函数来解决

![mark](http://myphoto.mtianyan.cn/blog/180331/BLDgBK3cA9.png?imageslim)

![mark](http://myphoto.mtianyan.cn/blog/180331/g9jml7gek4.png?imageslim)

在x和0中取最大值，这个函数在原点左侧部分斜率为0，右侧则是一条斜率为1的直线。

在第一象限导数恒为0.x在大于0时呈线性特点，小于0时则会是一条直线。

relu函数提供了非常良好的非线性特征。第一象限的这条直线表明它的导数是恒为1的。

x小于0这部分，导数恒为0.relu函数两个显而易见的优点: 

- 在第一象限不会有明显的梯度消失问题。因为导数恒为1

- 我们在初始化w时，因为是随机化，w有的会很大，有的会很小。连乘时就不会出现很大一直连乘，或者很小一直连乘。

- 求导代价小。

